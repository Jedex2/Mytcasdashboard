import asyncio
from playwright.async_api import async_playwright
import pandas as pd
from datetime import datetime

class EnhancedTCASScraper:
    def __init__(self):
        self.programs_data = []
        self.base_url = "https://course.mytcas.com"

    async def search_and_collect_programs(self, page, keyword):
        """р╕Др╣Йр╕Щр╕лр╕▓р╣Бр╕ер╕░р╕гр╕зр╕Ър╕гр╕зр╕бр╕лр╕ер╕▒р╕Бр╕кр╕╣р╕Хр╕г - р╣Гр╕Кр╣Йр╕зр╕┤р╕Шр╕╡р╕Чр╕╡р╣Ир╣Ар╕Йр╕Юр╕▓р╕░р╣Ар╕Ир╕▓р╕░р╕Ир╕З"""
        programs = []
        
        try:
            print(f"\nЁЯФН р╕Др╣Йр╕Щр╕лр╕▓: {keyword}")
            
            # р╣Др╕Ыр╕лр╕Щр╣Йр╕▓р╕лр╕ер╕▒р╕Б
            await page.goto(self.base_url, wait_until='networkidle', timeout=30000)
            await page.wait_for_timeout(2000)
            
            # р╣Гр╕Кр╣Й selector р╕Чр╕╡р╣Ир╣Ар╕Йр╕Юр╕▓р╕░р╣Ар╕Ир╕▓р╕░р╕Ир╕Зр╕Хр╕▓р╕бр╕Хр╕▒р╕зр╕нр╕вр╣Ир╕▓р╕З
            search_selectors = [
                "input[placeholder='р╕Юр╕┤р╕бр╕Юр╣Мр╕Кр╕╖р╣Ир╕нр╕бр╕лр╕▓р╕зр╕┤р╕Чр╕вр╕▓р╕ер╕▒р╕в р╕Др╕Ур╕░ р╕лр╕гр╕╖р╕нр╕лр╕ер╕▒р╕Бр╕кр╕╣р╕Хр╕г']",
                "input[placeholder*='р╕Др╣Йр╕Щр╕лр╕▓']",
                "input[type='search']",
                "input.search-input",
                "#search-input"
            ]
            
            search_input = None
            for selector in search_selectors:
                try:
                    search_input = await page.wait_for_selector(selector, timeout=5000)
                    if search_input:
                        print(f"  тЬЕ р╕Юр╕Ър╕Кр╣Ир╕нр╕Зр╕Др╣Йр╕Щр╕лр╕▓: {selector}")
                        break
                except:
                    continue
            
            if not search_input:
                print("тЭМ р╣Др╕бр╣Ир╕Юр╕Ър╕Кр╣Ир╕нр╕Зр╕Др╣Йр╕Щр╕лр╕▓")
                return []
            
            # р╕Чр╕│р╕Бр╕▓р╕гр╕Др╣Йр╕Щр╕лр╕▓
            await search_input.fill("")
            await page.wait_for_timeout(500)
            await search_input.fill(keyword)
            await search_input.press("Enter")
            await page.wait_for_timeout(3000)
            
            # р╕лр╕▓р╕Ьр╕ер╕ер╕▒р╕Юр╕Шр╣Мр╕Фр╣Йр╕зр╕в selector р╕лр╕ер╕▓р╕вр╣Бр╕Ър╕Ъ
            result_selectors = [
                ".t-programs > li",
                ".program-list li",
                ".search-results li",
                ".results li",
                "[data-testid='program-item']"
            ]
            
            results = []
            for selector in result_selectors:
                try:
                    results = await page.query_selector_all(selector)
                    if results:
                        print(f"  тЬЕ р╕Юр╕Ър╕Ьр╕ер╕ер╕▒р╕Юр╕Шр╣М: {selector} ({len(results)} р╕гр╕▓р╕вр╕Бр╕▓р╕г)")
                        break
                except:
                    continue
            
            if not results:
                print("  тЭМ р╣Др╕бр╣Ир╕Юр╕Ър╕Ьр╕ер╕ер╕▒р╕Юр╕Шр╣М")
                return []
            
            # р╕Ыр╕гр╕░р╕бр╕зр╕ер╕Ьр╕ер╕гр╕▓р╕вр╕Бр╕▓р╕гр╕Чр╕╡р╣Ир╕Юр╕Ъ
            for i, li in enumerate(results):
                try:
                    # р╕Фр╕╢р╕Зр╕Вр╣Йр╕нр╕бр╕╣р╕ер╕Юр╕╖р╣Йр╕Щр╕Рр╕▓р╕Щ
                    title_full = await li.inner_text()
                    
                    # р╕лр╕▓р╕ер╕┤р╕Зр╕Бр╣М
                    link_element = await li.query_selector("a")
                    if not link_element:
                        continue
                        
                    link = await link_element.get_attribute("href")
                    full_link = link if link.startswith("http") else f"{self.base_url}{link}"
                    
                    # р╣Бр╕вр╕Бр╕Вр╣Йр╕нр╕бр╕╣р╕ер╕Ир╕▓р╕Б title
                    lines = [line.strip() for line in title_full.strip().splitlines() if line.strip()]
                    
                    program_name = lines[0] if len(lines) >= 1 else ""
                    faculty = lines[1].replace('тА║', ' > ') if len(lines) >= 2 else ""
                    university = lines[2] if len(lines) >= 3 else ""
                    
                    programs.append({
                        'keyword': keyword,
                        'program_name': program_name,
                        'university': university,
                        'faculty': faculty,
                        'title_full': title_full,
                        'url': full_link
                    })
                    
                    print(f"  ЁЯУМ {i+1:2d}. {program_name[:40]}...")
                    
                except Exception as e:
                    print(f"  тЭМ р╕Вр╣Йр╕нр╕Ьр╕┤р╕Фр╕Юр╕ер╕▓р╕Фр╣Гр╕Щр╕гр╕▓р╕вр╕Бр╕▓р╕гр╕Чр╕╡р╣И {i+1}: {str(e)}")
                    continue
            
            return programs
            
        except Exception as e:
            print(f"тЭМ р╕Вр╣Йр╕нр╕Ьр╕┤р╕Фр╕Юр╕ер╕▓р╕Фр╣Гр╕Щр╕Бр╕▓р╕гр╕Др╣Йр╕Щр╕лр╕▓ {keyword}: {str(e)}")
            return []

    async def scrape_program_details(self, page, program_info):
        """р╕Фр╕╢р╕Зр╕Вр╣Йр╕нр╕бр╕╣р╕ер╕гр╕▓р╕вр╕ер╕░р╣Ар╕нр╕╡р╕вр╕Фр╕Ир╕▓р╕Бр╕лр╕Щр╣Йр╕▓р╕Вр╕нр╕Зр╣Бр╕Хр╣Ир╕ер╕░р╣Вр╕Ыр╕гр╣Бр╕Бр╕гр╕б"""
        url = program_info['url']
        
        try:
            print(f"ЁЯУД р╕Бр╕│р╕ер╕▒р╕Зр╕Фр╕╢р╕З: {program_info['program_name'][:50]}...")
            
            # р╣Ар╕Вр╣Йр╕▓р╕лр╕Щр╣Йр╕▓р╕гр╕▓р╕вр╕ер╕░р╣Ар╕нр╕╡р╕вр╕Ф
            await page.goto(url, wait_until='networkidle', timeout=30000)
            await page.wait_for_timeout(2000)
            
            # р╕кр╕гр╣Йр╕▓р╕Зр╕Вр╣Йр╕нр╕бр╕╣р╕ер╕Юр╕╖р╣Йр╕Щр╕Рр╕▓р╕Щ
            data = {
                'р╕Др╕│р╕Др╣Йр╕Щ': program_info['keyword'],
                'р╕Кр╕╖р╣Ир╕нр╕лр╕ер╕▒р╕Бр╕кр╕╣р╕Хр╕г': program_info['program_name'],
                'р╕бр╕лр╕▓р╕зр╕┤р╕Чр╕вр╕▓р╕ер╕▒р╕в': program_info['university'],
                'р╕Др╕Ур╕░': program_info['faculty'],
                'р╕Ыр╕гр╕░р╣Ар╕ар╕Чр╕лр╕ер╕▒р╕Бр╕кр╕╣р╕Хр╕г': 'р╣Др╕бр╣Ир╕Юр╕Ър╕Вр╣Йр╕нр╕бр╕╣р╕е',
                'р╕Др╣Ир╕▓р╣Гр╕Кр╣Йр╕Ир╣Ир╕▓р╕в': 'р╣Др╕бр╣Ир╕Юр╕Ър╕Вр╣Йр╕нр╕бр╕╣р╕е',
                'р╕ер╕┤р╕Зр╕Бр╣М': url,
                'р╕зр╕▒р╕Щр╕Чр╕╡р╣Ир╣Ар╕Бр╣Зр╕Ър╕Вр╣Йр╕нр╕бр╕╣р╕е': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            # р╕Фр╕╢р╕Зр╕Вр╣Йр╕нр╕бр╕╣р╕ер╕гр╕▓р╕вр╕ер╕░р╣Ар╕нр╕╡р╕вр╕Фр╕Фр╣Йр╕зр╕в selector р╕Чр╕╡р╣Ир╣Ар╕Йр╕Юр╕▓р╕░р╣Ар╕Ир╕▓р╕░р╕Ир╕З
            detail_selectors = {
                'р╕Ыр╕гр╕░р╣Ар╕ар╕Чр╕лр╕ер╕▒р╕Бр╕кр╕╣р╕Хр╕г': [
                    "dt:has-text('р╕Ыр╕гр╕░р╣Ар╕ар╕Чр╕лр╕ер╕▒р╕Бр╕кр╕╣р╕Хр╕г') + dd",
                    "td:has-text('р╕Ыр╕гр╕░р╣Ар╕ар╕Чр╕лр╕ер╕▒р╕Бр╕кр╕╣р╕Хр╕г') + td",
                    ".program-type",
                    "[data-field='program_type']"
                ],
                'р╕Др╣Ир╕▓р╣Гр╕Кр╣Йр╕Ир╣Ир╕▓р╕в': [
                    "dt:has-text('р╕Др╣Ир╕▓р╣Гр╕Кр╣Йр╕Ир╣Ир╕▓р╕в') + dd",
                    "dt:has-text('р╕Др╣Ир╕▓р╕Шр╕гр╕гр╕бр╣Ар╕Щр╕╡р╕вр╕б') + dd",
                    "td:has-text('р╕Др╣Ир╕▓р╣Гр╕Кр╣Йр╕Ир╣Ир╕▓р╕в') + td",
                    ".fee-info",
                    ".tuition-fee",
                    "[data-field='fee']"
                ]
            }
            
            # р╕ер╕нр╕Зр╕Фр╕╢р╕Зр╕Вр╣Йр╕нр╕бр╕╣р╕ер╣Бр╕Хр╣Ир╕ер╕░р╕Яр╕┤р╕ер╕Фр╣М
            for field, selectors in detail_selectors.items():
                for selector in selectors:
                    try:
                        element = await page.query_selector(selector)
                        if element:
                            text = await element.inner_text()
                            if text.strip():
                                data[field] = text.strip()
                                break
                    except:
                        continue
            
            # р╕Фр╕╢р╕Зр╕Вр╣Йр╕нр╕бр╕╣р╕ер╣Ар╕Юр╕┤р╣Ир╕бр╣Ар╕Хр╕┤р╕бр╕Ир╕▓р╕Бр╕Хр╕▓р╕гр╕▓р╕З (р╕лр╕▓р╕Бр╕бр╕╡)
            try:
                table_rows = await page.query_selector_all("table tr")
                for row in table_rows:
                    cells = await row.query_selector_all("td, th")
                    if len(cells) >= 2:
                        header = await cells[0].inner_text()
                        value = await cells[1].inner_text()
                        
                        if "р╕Ыр╕гр╕░р╣Ар╕ар╕Ч" in header and data['р╕Ыр╕гр╕░р╣Ар╕ар╕Чр╕лр╕ер╕▒р╕Бр╕кр╕╣р╕Хр╕г'] == 'р╣Др╕бр╣Ир╕Юр╕Ър╕Вр╣Йр╕нр╕бр╕╣р╕е':
                            data['р╕Ыр╕гр╕░р╣Ар╕ар╕Чр╕лр╕ер╕▒р╕Бр╕кр╕╣р╕Хр╕г'] = value.strip()
                        elif any(word in header for word in ['р╕Др╣Ир╕▓р╣Гр╕Кр╣Йр╕Ир╣Ир╕▓р╕в', 'р╕Шр╕гр╕гр╕бр╣Ар╕Щр╕╡р╕вр╕б', 'р╕Др╣Ир╕▓р╣Ар╕гр╕╡р╕вр╕Щ']) and data['р╕Др╣Ир╕▓р╣Гр╕Кр╣Йр╕Ир╣Ир╕▓р╕в'] == 'р╣Др╕бр╣Ир╕Юр╕Ър╕Вр╣Йр╕нр╕бр╕╣р╕е':
                            data['р╕Др╣Ир╕▓р╣Гр╕Кр╣Йр╕Ир╣Ир╕▓р╕в'] = value.strip()
            except:
                pass
            
            print(f"   тЬЕ {data['р╕бр╕лр╕▓р╕зр╕┤р╕Чр╕вр╕▓р╕ер╕▒р╕в'][:25]} - {data['р╕Др╣Ир╕▓р╣Гр╕Кр╣Йр╕Ир╣Ир╕▓р╕в'][:30]}...")
            return data
            
        except Exception as e:
            print(f"   тЭМ р╕Вр╣Йр╕нр╕Ьр╕┤р╕Фр╕Юр╕ер╕▓р╕Ф: {str(e)}")
            return None

    async def run_scraping(self, keywords=None):
        """р╣Ар╕гр╕╡р╕вр╕Бр╣Гр╕Кр╣Йр╕Бр╕▓р╕г scraping"""
        if keywords is None:
            keywords = ["р╕зр╕┤р╕ир╕зр╕Бр╕гр╕гр╕б р╕Ыр╕▒р╕Нр╕Нр╕▓р╕Ыр╕гр╕░р╕Фр╕┤р╕йр╕Рр╣М", "р╕зр╕┤р╕ир╕зр╕Бр╕гр╕гр╕б р╕Др╕нр╕бр╕Юр╕┤р╕зр╣Ар╕Хр╕нр╕гр╣М"]
        
        print("ЁЯЪА р╣Ар╕гр╕┤р╣Ир╕б Enhanced TCAS Scraper")
        print("="*70)
        print("ЁЯФз р╕Ыр╕гр╕▒р╕Ър╕Ыр╕гр╕╕р╕Зр╕Хр╕▓р╕бр╕Хр╕▒р╕зр╕нр╕вр╣Ир╕▓р╕З:")
        print("   тЬЕ р╣Гр╕Кр╣Й selector р╕Чр╕╡р╣Ир╣Ар╕Йр╕Юр╕▓р╕░р╣Ар╕Ир╕▓р╕░р╕Ир╕З")
        print("   тЬЕ р╣Ар╕Вр╣Йр╕▓р╣Др╕Ыр╕Фр╕╢р╕Зр╕Вр╣Йр╕нр╕бр╕╣р╕ер╕гр╕▓р╕вр╕ер╕░р╣Ар╕нр╕╡р╕вр╕Фр╕Ир╕▓р╕Бр╕лр╕Щр╣Йр╕▓р╣Бр╕Хр╣Ир╕ер╕░р╣Вр╕Ыр╕гр╣Бр╕Бр╕гр╕б")
        print("   тЬЕ р╣Гр╕Кр╣Йр╣Вр╕Др╕гр╕Зр╕кр╕гр╣Йр╕▓р╕З HTML р╣Бр╕Чр╕Щ regex")
        print("   тЬЕ р╕бр╕╡ fallback selector р╕лр╕ер╕▓р╕вр╕Хр╕▒р╕з")
        print("="*70)
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=False)
            context = await browser.new_context(
                locale='th-TH',
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            )
            page = await context.new_page()
            
            try:
                all_programs = []
                
                # р╕Вр╕▒р╣Йр╕Щр╕Хр╕нр╕Щр╕Чр╕╡р╣И 1: р╕гр╕зр╕Ър╕гр╕зр╕бр╕ер╕┤р╕Зр╕Бр╣Мр╕Чр╕▒р╣Йр╕Зр╕лр╕бр╕Ф
                for keyword in keywords:
                    programs = await self.search_and_collect_programs(page, keyword)
                    all_programs.extend(programs)
                    await asyncio.sleep(2)
                
                if not all_programs:
                    print("тЭМ р╣Др╕бр╣Ир╕Юр╕Ър╕лр╕ер╕▒р╕Бр╕кр╕╣р╕Хр╕гр╣Гр╕Фр╣Ж")
                    return 0
                
                print(f"\nЁЯУЛ р╣Ар╕гр╕┤р╣Ир╕бр╕Фр╕╢р╕Зр╕Вр╣Йр╕нр╕бр╕╣р╕ер╕гр╕▓р╕вр╕ер╕░р╣Ар╕нр╕╡р╕вр╕Ф {len(all_programs)} р╕лр╕ер╕▒р╕Бр╕кр╕╣р╕Хр╕г...")
                
                # р╕Вр╕▒р╣Йр╕Щр╕Хр╕нр╕Щр╕Чр╕╡р╣И 2: р╕Фр╕╢р╕Зр╕Вр╣Йр╕нр╕бр╕╣р╕ер╕гр╕▓р╕вр╕ер╕░р╣Ар╕нр╕╡р╕вр╕Фр╣Бр╕Хр╣Ир╕ер╕░р╕лр╕ер╕▒р╕Бр╕кр╕╣р╕Хр╕г
                for i, program_info in enumerate(all_programs, 1):
                    print(f"\n[{i:2d}/{len(all_programs)}]", end=" ")
                    
                    data = await self.scrape_program_details(page, program_info)
                    if data:
                        self.programs_data.append(data)
                    
                    # р╕лр╕Щр╣Ир╕зр╕Зр╣Ар╕зр╕ер╕▓р╕Ыр╣Йр╕нр╕Зр╕Бр╕▒р╕Щ rate limiting
                    await asyncio.sleep(1.5)
                
            finally:
                await browser.close()
        
        return len(self.programs_data)

    def save_to_excel(self, filename='enhanced_tcas_data'):
        """р╕Ър╕▒р╕Щр╕Чр╕╢р╕Бр╣Ар╕Ыр╣Зр╕Щ Excel"""
        if not self.programs_data:
            print("тЭМ р╣Др╕бр╣Ир╕бр╕╡р╕Вр╣Йр╕нр╕бр╕╣р╕ер╕Чр╕╡р╣Ир╕Ир╕░р╕Ър╕▒р╕Щр╕Чр╕╢р╕Б")
            return None
        
        df = pd.DataFrame(self.programs_data)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"{filename}_{timestamp}.xlsx"
        
        # р╕Ър╕▒р╕Щр╕Чр╕╢р╕Бр╣Ар╕Ыр╣Зр╕Щ Excel
        df.to_excel(filename, index=False, engine='openpyxl')
        
        print(f"\nЁЯТ╛ р╕Ър╕▒р╕Щр╕Чр╕╢р╕Бр╣Ар╕гр╕╡р╕вр╕Ър╕гр╣Йр╕нр╕в: {filename}")
        print(f"ЁЯУК р╕Ир╕│р╕Щр╕зр╕Щр╕Вр╣Йр╕нр╕бр╕╣р╕е: {len(df)} р╕гр╕▓р╕вр╕Бр╕▓р╕г")
        
        # р╣Бр╕кр╕Фр╕Зр╕кр╕гр╕╕р╕Ы
        if len(df) > 0:
            print(f"\nЁЯУИ р╕кр╕гр╕╕р╕Ыр╕Хр╕▓р╕бр╕Др╕│р╕Др╣Йр╕Щ:")
            keyword_counts = df['р╕Др╕│р╕Др╣Йр╕Щ'].value_counts()
            for keyword, count in keyword_counts.items():
                emoji = "ЁЯдЦ" if "р╕Ыр╕▒р╕Нр╕Нр╕▓р╕Ыр╕гр╕░р╕Фр╕┤р╕йр╕Рр╣М" in keyword else "ЁЯТ╗"
                print(f"   {emoji} {keyword}: {count} р╕гр╕▓р╕вр╕Бр╕▓р╕г")
            
            # р╕Щр╕▒р╕Ър╕Чр╕╡р╣Ир╕бр╕╡р╕Др╣Ир╕▓р╣Гр╕Кр╣Йр╕Ир╣Ир╕▓р╕в
            with_fee = len(df[df['р╕Др╣Ир╕▓р╣Гр╕Кр╣Йр╕Ир╣Ир╕▓р╕в'] != 'р╣Др╕бр╣Ир╕Юр╕Ър╕Вр╣Йр╕нр╕бр╕╣р╕е'])
            print(f"\nЁЯТ░ р╕бр╕╡р╕Вр╣Йр╕нр╕бр╕╣р╕ер╕Др╣Ир╕▓р╣Гр╕Кр╣Йр╕Ир╣Ир╕▓р╕в: {with_fee}/{len(df)} р╕гр╕▓р╕вр╕Бр╕▓р╕г")
            
            # р╣Бр╕кр╕Фр╕Зр╕Хр╕▒р╕зр╕нр╕вр╣Ир╕▓р╕Зр╕Вр╣Йр╕нр╕бр╕╣р╕е
            print(f"\nЁЯУЛ р╕Хр╕▒р╕зр╕нр╕вр╣Ир╕▓р╕Зр╕Вр╣Йр╕нр╕бр╕╣р╕е:")
            for i, row in df.head(3).iterrows():
                print(f"   {i+1}. {row['р╕Кр╕╖р╣Ир╕нр╕лр╕ер╕▒р╕Бр╕кр╕╣р╕Хр╕г'][:40]}...")
                print(f"      ЁЯПл {row['р╕бр╕лр╕▓р╕зр╕┤р╕Чр╕вр╕▓р╕ер╕▒р╕в']}")
                print(f"      ЁЯТ░ {row['р╕Др╣Ир╕▓р╣Гр╕Кр╣Йр╕Ир╣Ир╕▓р╕в']}")
        
        return df

    def save_to_csv(self, filename='enhanced_tcas_data'):
        """р╕Ър╕▒р╕Щр╕Чр╕╢р╕Бр╣Ар╕Ыр╣Зр╕Щ CSV р╕кр╕│р╕гр╕нр╕З"""
        if not self.programs_data:
            return None
        
        df = pd.DataFrame(self.programs_data)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"{filename}_{timestamp}.csv"
        
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"ЁЯТ╛ р╕Ър╕▒р╕Щр╕Чр╕╢р╕Б CSV р╕кр╕│р╕гр╕нр╕З: {filename}")
        return df

async def main():
    """р╕Яр╕▒р╕Зр╕Бр╣Мр╕Кр╕▒р╕Щр╕лр╕ер╕▒р╕Б"""
    print("ЁЯОп Enhanced TCAS Scraper")
    print("ЁЯУЪ р╕Ыр╕гр╕▒р╕Ър╕Ыр╕гр╕╕р╕Зр╕Ир╕▓р╕Бр╕Хр╕▒р╕зр╕нр╕вр╣Ир╕▓р╕Зр╕Чр╕╡р╣Ир╣Гр╕лр╣Йр╕бр╕▓")
    print("="*50)
    
    # р╣Гр╕лр╣Йр╕Ьр╕╣р╣Йр╣Гр╕Кр╣Йр╣Ар╕ер╕╖р╕нр╕Бр╕Др╕│р╕Др╣Йр╕Щр╕лр╕▓
    print("ЁЯФН р╣Ар╕ер╕╖р╕нр╕Бр╕Др╕│р╕Др╣Йр╕Щр╕лр╕▓:")
    print("1. р╕зр╕┤р╕ир╕зр╕Бр╕гр╕гр╕б р╕Ыр╕▒р╕Нр╕Нр╕▓р╕Ыр╕гр╕░р╕Фр╕┤р╕йр╕Рр╣М")
    print("2. р╕зр╕┤р╕ир╕зр╕Бр╕гр╕гр╕б р╕Др╕нр╕бр╕Юр╕┤р╕зр╣Ар╕Хр╕нр╕гр╣М")
    print("3. р╕Чр╕▒р╣Йр╕Зр╕кр╕нр╕Зр╕Др╕│")
    print("4. р╕Бр╕│р╕лр╕Щр╕Фр╣Ар╕нр╕З")
    
    choice = input("р╣Ар╕ер╕╖р╕нр╕Б (1-4): ").strip()
    
    keywords = []
    if choice == "1":
        keywords = ["р╕зр╕┤р╕ир╕зр╕Бр╕гр╕гр╕б р╕Ыр╕▒р╕Нр╕Нр╕▓р╕Ыр╕гр╕░р╕Фр╕┤р╕йр╕Рр╣М"]
    elif choice == "2":
        keywords = ["р╕зр╕┤р╕ир╕зр╕Бр╕гр╕гр╕б р╕Др╕нр╕бр╕Юр╕┤р╕зр╣Ар╕Хр╕нр╕гр╣М"]
    elif choice == "3":
        keywords = ["р╕зр╕┤р╕ир╕зр╕Бр╕гр╕гр╕б р╕Ыр╕▒р╕Нр╕Нр╕▓р╕Ыр╕гр╕░р╕Фр╕┤р╕йр╕Рр╣М", "р╕зр╕┤р╕ир╕зр╕Бр╕гр╕гр╕б р╕Др╕нр╕бр╕Юр╕┤р╕зр╣Ар╕Хр╕нр╕гр╣М"]
    elif choice == "4":
        custom_keywords = input("р╕Бр╕гр╕нр╕Бр╕Др╕│р╕Др╣Йр╕Щр╕лр╕▓ (р╕Др╕▒р╣Ир╕Щр╕Фр╣Йр╕зр╕вр╣Ар╕Др╕гр╕╖р╣Ир╕нр╕Зр╕лр╕бр╕▓р╕вр╕Ир╕╕р╕ер╕ар╕▓р╕Д): ")
        keywords = [k.strip() for k in custom_keywords.split(',') if k.strip()]
    else:
        keywords = ["р╕зр╕┤р╕ир╕зр╕Бр╕гр╕гр╕б р╕Ыр╕▒р╕Нр╕Нр╕▓р╕Ыр╕гр╕░р╕Фр╕┤р╕йр╕Рр╣М"]
    
    print(f"ЁЯОп р╕Ир╕░р╕Др╣Йр╕Щр╕лр╕▓: {', '.join(keywords)}")
    print("="*50)
    
    scraper = EnhancedTCASScraper()
    
    try:
        # р╣Ар╕гр╕┤р╣Ир╕бр╕Бр╕▓р╕г scraping
        found_count = await scraper.run_scraping(keywords)
        
        if found_count > 0:
            print(f"\nЁЯОЙ р╣Ар╕кр╕гр╣Зр╕Ир╕кр╕┤р╣Йр╕Щ! р╕Фр╕╢р╕Зр╕Вр╣Йр╕нр╕бр╕╣р╕ер╣Др╕Фр╣Й {found_count} р╕лр╕ер╕▒р╕Бр╕кр╕╣р╕Хр╕г")
            
            # р╕Ър╕▒р╕Щр╕Чр╕╢р╕Бр╕Вр╣Йр╕нр╕бр╕╣р╕е
            df = scraper.save_to_excel()
            scraper.save_to_csv()  # р╕кр╕│р╕гр╕нр╕З
            
            print("\nтЬЕ р╣Др╕Яр╕ер╣Мр╕Юр╕гр╣Йр╕нр╕бр╣Гр╕Кр╣Йр╕Зр╕▓р╕Щ!")
            
            if df is not None and len(df) > 0:
                print(f"\nЁЯУК р╕кр╕Цр╕┤р╕Хр╕┤р╕гр╕зр╕б:")
                print(f"   ЁЯУЪ р╕Ир╕│р╕Щр╕зр╕Щр╕лр╕ер╕▒р╕Бр╕кр╕╣р╕Хр╕гр╕Чр╕▒р╣Йр╕Зр╕лр╕бр╕Ф: {len(df)}")
                print(f"   ЁЯПл р╕Ир╕│р╕Щр╕зр╕Щр╕бр╕лр╕▓р╕зр╕┤р╕Чр╕вр╕▓р╕ер╕▒р╕в: {df['р╕бр╕лр╕▓р╕зр╕┤р╕Чр╕вр╕▓р╕ер╕▒р╕в'].nunique()}")
                print(f"   ЁЯТ░ р╕бр╕╡р╕Вр╣Йр╕нр╕бр╕╣р╕ер╕Др╣Ир╕▓р╣Гр╕Кр╣Йр╕Ир╣Ир╕▓р╕в: {len(df[df['р╕Др╣Ир╕▓р╣Гр╕Кр╣Йр╕Ир╣Ир╕▓р╕в'] != 'р╣Др╕бр╣Ир╕Юр╕Ър╕Вр╣Йр╕нр╕бр╕╣р╕е'])}")
        else:
            print("\nтЭМ р╣Др╕бр╣Ир╕бр╕╡р╕Вр╣Йр╕нр╕бр╕╣р╕ер╕Чр╕╡р╣Ир╕Фр╕╢р╕Зр╣Др╕Фр╣Й")
    
    except KeyboardInterrupt:
        print("\nтП╣я╕П р╕лр╕вр╕╕р╕Фр╕Бр╕▓р╕гр╕Чр╕│р╕Зр╕▓р╕Щр╣Вр╕Фр╕вр╕Ьр╕╣р╣Йр╣Гр╕Кр╣Й")
    except Exception as e:
        print(f"\nтЭМ р╣Ар╕Бр╕┤р╕Фр╕Вр╣Йр╕нр╕Ьр╕┤р╕Фр╕Юр╕ер╕▓р╕Ф: {str(e)}")
        import traceback
        print(f"ЁЯУЛ р╕гр╕▓р╕вр╕ер╕░р╣Ар╕нр╕╡р╕вр╕Ф: {traceback.format_exc()}")

if __name__ == "__main__":
    asyncio.run(main())